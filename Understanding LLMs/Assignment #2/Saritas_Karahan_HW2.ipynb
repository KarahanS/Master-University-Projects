{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIyOyRg5FO2u"
      },
      "source": [
        "# Exercise #1\n",
        "\n",
        "* Consider the following language models: GPT-2, GPT-4, Vicuna (an instruction-tuned version of Llama) and Llama-2-7b-base.\n",
        "* Consider the following prompting / generation strategies: Beam search, tree-of-thought reasoning, zero-shot CoT prompting, few-shot CoT prompting, few-shot prompting.\n",
        "\n",
        "For each model, which strategies do you think work well, and why? Do you think there are particular tasks or contexts, in which they work better, than in others?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1a1LBkLe5cO"
      },
      "source": [
        "Let's start by briefly describing each model and technique.\n",
        "* GPT-2: A large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages, called WebText (an internal OpenAI corpus created by scraping web pages with emphasis on document quality).\n",
        "* GPT-4: A multimodal transformer-based language model, fine-tuned with reinforcement learning from human and AI feedback.\n",
        "* Vicuna: An open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.\n",
        "* Llama-2-7b-base: A large transformer-based open-source language model with 7 billion parameters, trained on a mix of data from publicly available sources.\n",
        "\n",
        "Techniques:\n",
        "* Beam search: A search algorithm for text generation that maintains $k$ path probabilities at each decision point, discarding the least likely paths as it progresses. The idea is to avoid eliminating all other possibilities at each step, instead keeping a few alternatives to explore, as one of them might turn out to be the most likely output.\n",
        "* Tree-of-thought reasoning: A technique that structures the generation process as a tree, where each node represents a thought or concept. It is used to guide the model's generation process by enforcing logical consistency and coherence.\n",
        "* Few-shot prompting: A technique that prompts the LM with $k$ pairs of demonstrations $(x_i, y_i)$ to learn a task-specific mapping from input to output.\n",
        "* Few-shot CoT prompting: A variant of the few-shot prompting where the LM is requested to provide chain-of-thought reasoning. This way, the model is expected to generate more accurate and coherent responses. The idea is to introduce a chain-of-thought to bridge the input $x$ to output $y$ when that connection is non-trivial.\n",
        "* Zero-shot CoT prompting: A technique that prompts the LM with a chain-of-thought reasoning task without any demonstrations (as in the few-shot prompting).\n",
        "\n",
        "Comments:\n",
        "\n",
        "* Beam search is a widely used decoding strategy for text generation, but it can be prone to repetition and lack of diversity in the generated text. It is particularly useful for machine translation [[1]](https://arxiv.org/abs/1808.09582) [[2]](https://arxiv.org/abs/1808.10006). I believe it can increase the performances of the models - but the improvement would be upper-bounded by the model's capabilities.\n",
        "\n",
        "* GPT-2's relatively small architecture limits its zero-shot performance on most tasks. It is shown that while GPT-2 can match supervised baselines in zero-shot reading comprehension (still way lower than the human level), its performance in summarization and other tasks like translation and question answering remains rudimentary compared to established techniques [[3]](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf). Few shot and beam search should work well with GPT-2.\n",
        "\n",
        "* It is shown that the larger models with high number of parameters, make increasingly efficient use of in-context information [[4]](https://arxiv.org/abs/2005.14165). Therefore the strategies such as few-shot prompting and few-shot CoT prompting would have more impact on the performance of GPT-4 (estimated to have ~1.8 trillion parameters[*](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/)), then on Vicuna (assuming we use Vicuna-v1.5 based on Llama2, other versions are based on Llama1), Llama-2-7b-base (7 billion parameters) and GPT-2 (1.5 billion parameters) respectively. Zero shot CoT should work well with Vicuna, because it was fine-tuned on instructions.\n",
        "The same hierarchy applies to zero-shot chain-of-thought (CoT) prompting, but the gap is narrower when no example pair is provided, as shown in [[4]](https://arxiv.org/abs/2005.14165), [[5]](https://arxiv.org/abs/2205.11916).\n",
        "\n",
        "\n",
        "* ToT is a complex technique that relies heavily on the \"thoughts\" generated by the LM, making it most beneficial for higher-capability models like GPT-4. It is recommended to be used in tasks requiring deliberate reasoning, such as mathematical problems, where CoT struggles [[6]](https://arxiv.org/abs/2305.10601). Surprisingly, ToT might also boost the performance of models like Vicuna or Llama-2-7b-base, as GPT-3.5 ToT has been shown to outperform GPT-4 Input-Output on various tasks [[6]](https://arxiv.org/abs/2305.10601).\n",
        "\n",
        "* In their blog post[*](https://lmsys.org/blog/2023-03-30-vicuna/), the Vicuna-13b Team shows that Vicuna, fine-tuned on 70,000 user-shared ChatGPT conversations, can compete with GPT-3.5 in areas like Humanities and writing, with GPT-4 serving as the judge [[7]](https://arxiv.org/abs/2306.05685). However, Vicuna still lags behind GPT-3.5 in coding, mathematical questions, and reasoning. In summary, LLaMa-1-13b is significantly outperformed in all fields by Vicuna-13b-v1.3, which can compete with Llama-2 models on most tasks (but slightly worse) and with GPT-3.5 on writing and humanities, though GPT-3.5 decisively outperforms both Vicuna-13b-v1.3 and Llama-2 in reasoning and mathematical tasks. Surprisingly, Vicuna-33b-v1.3 (trained on LLama-1) is slightly better than Llama-2 models with 7b and 13b parameters - and slightly worse than Llama-2-70b-chat[*](https://chat.lmsys.org/?leaderboard).\n",
        "\n",
        "* ChatGPT Llama-2 models are shown to be slightly outperformed by GPT-3.5 in benchmarks [[8]](https://arxiv.org/pdf/2307.09288), making GPT-4 the best model among the list. However, Llama-2 and Vicuna models are preferable for tasks requiring fine-tuning or architectural modifications, as they are open-source. Although Llama models currently perform worse than GPT, their open-source nature makes them appealing for advanced usage and customization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDg7B-FeiGZ0"
      },
      "source": [
        "Vicuna versions (source: https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md)\n",
        "\n",
        "| Weights version | Link | FastChat version compatibility | Base Model | Release Date | Fine-tuning Data |\n",
        "| ---- | ---- | ---- | ---- | ---- | ---- |\n",
        "| v1.5 | [7B](https://huggingface.co/lmsys/vicuna-7b-v1.5), [7B-16k](https://huggingface.co/lmsys/vicuna-7b-v1.5-16k), [13B](https://huggingface.co/lmsys/vicuna-13b-v1.5), [13B-16k](https://huggingface.co/lmsys/vicuna-13b-v1.5-16k) | `>=0.2.21` | Llama 2 | Aug. 1, 2023 | 370M tokens |\n",
        "| v1.3 | [7B](https://huggingface.co/lmsys/vicuna-7b-v1.3), [13B](https://huggingface.co/lmsys/vicuna-13b-v1.3), [33B](//huggingface.co/lmsys/vicuna-33b-v1.3) | `>=0.2.1` | Llama 1 | Jun. 22, 2023 | 370M tokens |\n",
        "| v1.1 | [7B](https://huggingface.co/lmsys/vicuna-7b-v1.1), [13B](https://huggingface.co/lmsys/vicuna-13b-v1.1) | `>=0.2.1` | Llama 1 | Apr. 12, 2023 | - |\n",
        "| v0 | [7B-delta](https://huggingface.co/lmsys/vicuna-7b-delta-v0), [13B-delta](https://huggingface.co/lmsys/vicuna-13b-delta-v0) | `<=0.1.10` | Llama 1 | Mar. 30, 2023 | - |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "id": "V4PFnjKOe7NL",
        "outputId": "c6f86eb7-a7b3-4f85-8b14-209a62cf41e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-05-30 08:46:40--  https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_judgment/gpt-4_single.jsonl\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.88, 18.172.134.24, 18.172.134.124, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/12/2b/122bd8e9eccbb3acc98acf73e0ecef3c96f24dcdb5f6639074ed304eb19f9cd4/76c55033c6b2b1cc3f62513458f84748a23352495fd42b1062a7401de5ff9bd9?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27gpt-4_single.jsonl%3B+filename%3D%22gpt-4_single.jsonl%22%3B&Expires=1717318001&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNzMxODAwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8xMi8yYi8xMjJiZDhlOWVjY2JiM2FjYzk4YWNmNzNlMGVjZWYzYzk2ZjI0ZGNkYjVmNjYzOTA3NGVkMzA0ZWIxOWY5Y2Q0Lzc2YzU1MDMzYzZiMmIxY2MzZjYyNTEzNDU4Zjg0NzQ4YTIzMzUyNDk1ZmQ0MmIxMDYyYTc0MDFkZTVmZjliZDk%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=DaMbuf3bBm2W0LYtV7sqNT4yIXdJ5S4SafZLvqFFHA0DQc9Mw-TMgqa1eO7xOg2SDXOZYrTMug6sQnEvABmGVjx-q18QNbVzT-6Em9%7E%7EogW6iEDxHhadkJeRIHBFreyZbhXGzqTOvQycISN0ZGa9DXGsCcsHDjLBlO7eZAHZQS516mVZ-wmuzoZzrCimtGQ%7Euj4Z1vOQNUTCFbB-nTA7N7QTb8DhtSQ1KX7zJk9Kna6c4f5ocRTyQn5ojeXl4EdwTq8fZU5uk62XwMO3e8RfoG6%7E1BG4WzYLORfKOU4wZu1EtK0IWdwdNLD29-pIzpiEZZGY4kf3cU7y2eMUsclaNw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-05-30 08:46:41--  https://cdn-lfs.huggingface.co/repos/12/2b/122bd8e9eccbb3acc98acf73e0ecef3c96f24dcdb5f6639074ed304eb19f9cd4/76c55033c6b2b1cc3f62513458f84748a23352495fd42b1062a7401de5ff9bd9?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27gpt-4_single.jsonl%3B+filename%3D%22gpt-4_single.jsonl%22%3B&Expires=1717318001&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNzMxODAwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8xMi8yYi8xMjJiZDhlOWVjY2JiM2FjYzk4YWNmNzNlMGVjZWYzYzk2ZjI0ZGNkYjVmNjYzOTA3NGVkMzA0ZWIxOWY5Y2Q0Lzc2YzU1MDMzYzZiMmIxY2MzZjYyNTEzNDU4Zjg0NzQ4YTIzMzUyNDk1ZmQ0MmIxMDYyYTc0MDFkZTVmZjliZDk%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=DaMbuf3bBm2W0LYtV7sqNT4yIXdJ5S4SafZLvqFFHA0DQc9Mw-TMgqa1eO7xOg2SDXOZYrTMug6sQnEvABmGVjx-q18QNbVzT-6Em9%7E%7EogW6iEDxHhadkJeRIHBFreyZbhXGzqTOvQycISN0ZGa9DXGsCcsHDjLBlO7eZAHZQS516mVZ-wmuzoZzrCimtGQ%7Euj4Z1vOQNUTCFbB-nTA7N7QTb8DhtSQ1KX7zJk9Kna6c4f5ocRTyQn5ojeXl4EdwTq8fZU5uk62XwMO3e8RfoG6%7E1BG4WzYLORfKOU4wZu1EtK0IWdwdNLD29-pIzpiEZZGY4kf3cU7y2eMUsclaNw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.185.64, 18.154.185.94, 18.154.185.26, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.185.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20113128 (19M) [text/plain]\n",
            "Saving to: ‘gpt-4_single.jsonl’\n",
            "\n",
            "gpt-4_single.jsonl  100%[===================>]  19.18M  84.7MB/s    in 0.2s    \n",
            "\n",
            "2024-05-30 08:46:41 (84.7 MB/s) - ‘gpt-4_single.jsonl’ saved [20113128/20113128]\n",
            "\n",
            "--2024-05-30 08:46:41--  https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_judgment/gpt-4_pair.jsonl\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.4, 18.172.134.124, 18.172.134.24, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/12/2b/122bd8e9eccbb3acc98acf73e0ecef3c96f24dcdb5f6639074ed304eb19f9cd4/d662c0b7d1d297f0494fcb4cc09fe8f054fa22d75deb4754a483a921984bc585?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27gpt-4_pair.jsonl%3B+filename%3D%22gpt-4_pair.jsonl%22%3B&Expires=1717318001&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNzMxODAwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8xMi8yYi8xMjJiZDhlOWVjY2JiM2FjYzk4YWNmNzNlMGVjZWYzYzk2ZjI0ZGNkYjVmNjYzOTA3NGVkMzA0ZWIxOWY5Y2Q0L2Q2NjJjMGI3ZDFkMjk3ZjA0OTRmY2I0Y2MwOWZlOGYwNTRmYTIyZDc1ZGViNDc1NGE0ODNhOTIxOTg0YmM1ODU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=c7OeCIn2i-Heb2Y9Q3JTaHziRcV0RdGthHcbC%7EcmzfxCnKX6I7Cs5Jk-MKcm07Tsa0pVUDm0H5XisfBphFKGROr0t3PRt9ZE5a3WqhG-g1v4GP7oVOyN4j7UEL7BGr0iA-dpyUcLQulamF9ui%7EHJ5W6QoUXHusueEiMJ1-S8VsvyKbZb2R5%7EXYLOsBM5NOvLL4IDZPTaOHLCUsQdeSBxK2Rf1%7El1rOQ-BJdPpLxvPTN7whnFytOrFA-EenJ%7ECDI39xxWFLLcIkDV3L03ciMs5GzYhCRqaDIs49Ar1Wbf60S8OOyc7ROUjUyGWHAbmvbbmR9md09n3DdDKqQmW79%7Emg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-05-30 08:46:41--  https://cdn-lfs.huggingface.co/repos/12/2b/122bd8e9eccbb3acc98acf73e0ecef3c96f24dcdb5f6639074ed304eb19f9cd4/d662c0b7d1d297f0494fcb4cc09fe8f054fa22d75deb4754a483a921984bc585?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27gpt-4_pair.jsonl%3B+filename%3D%22gpt-4_pair.jsonl%22%3B&Expires=1717318001&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNzMxODAwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8xMi8yYi8xMjJiZDhlOWVjY2JiM2FjYzk4YWNmNzNlMGVjZWYzYzk2ZjI0ZGNkYjVmNjYzOTA3NGVkMzA0ZWIxOWY5Y2Q0L2Q2NjJjMGI3ZDFkMjk3ZjA0OTRmY2I0Y2MwOWZlOGYwNTRmYTIyZDc1ZGViNDc1NGE0ODNhOTIxOTg0YmM1ODU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=c7OeCIn2i-Heb2Y9Q3JTaHziRcV0RdGthHcbC%7EcmzfxCnKX6I7Cs5Jk-MKcm07Tsa0pVUDm0H5XisfBphFKGROr0t3PRt9ZE5a3WqhG-g1v4GP7oVOyN4j7UEL7BGr0iA-dpyUcLQulamF9ui%7EHJ5W6QoUXHusueEiMJ1-S8VsvyKbZb2R5%7EXYLOsBM5NOvLL4IDZPTaOHLCUsQdeSBxK2Rf1%7El1rOQ-BJdPpLxvPTN7whnFytOrFA-EenJ%7ECDI39xxWFLLcIkDV3L03ciMs5GzYhCRqaDIs49Ar1Wbf60S8OOyc7ROUjUyGWHAbmvbbmR9md09n3DdDKqQmW79%7Emg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.185.64, 18.154.185.94, 18.154.185.26, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.185.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 48043462 (46M) [binary/octet-stream]\n",
            "Saving to: ‘gpt-4_pair.jsonl’\n",
            "\n",
            "gpt-4_pair.jsonl    100%[===================>]  45.82M  32.1MB/s    in 1.4s    \n",
            "\n",
            "2024-05-30 08:46:43 (32.1 MB/s) - ‘gpt-4_pair.jsonl’ saved [48043462/48043462]\n",
            "\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
            "Collecting plotly\n",
            "  Downloading plotly-5.22.0-py3-none-any.whl (16.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (24.0)\n",
            "Installing collected packages: kaleido, plotly\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.15.0\n",
            "    Uninstalling plotly-5.15.0:\n",
            "      Successfully uninstalled plotly-5.15.0\n",
            "Successfully installed kaleido-0.2.1 plotly-5.22.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "ffcd467eeeb145939b1058405d9c168d",
              "pip_warning": {
                "packages": [
                  "_plotly_utils",
                  "plotly"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!wget https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_judgment/gpt-4_single.jsonl\n",
        "!wget https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_judgment/gpt-4_pair.jsonl\n",
        "!pip install -U plotly kaleido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705
        },
        "id": "PxMPMvjofpCR",
        "outputId": "8649c7e1-6416-4c3b-ec51-5f94647f2da1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['alpaca-13b' 'baize-v2-13b' 'chatglm-6b' 'claude-instant-v1' 'claude-v1'\n",
            " 'dolly-v2-12b' 'falcon-40b-instruct' 'fastchat-t5-3b' 'gpt-3.5-turbo'\n",
            " 'gpt-4' 'gpt4all-13b-snoozy' 'guanaco-33b' 'guanaco-65b'\n",
            " 'h2ogpt-oasst-open-llama-13b' 'koala-13b' 'llama-13b' 'mpt-30b-chat'\n",
            " 'mpt-30b-instruct' 'mpt-7b-chat' 'nous-hermes-13b'\n",
            " 'oasst-sft-4-pythia-12b' 'oasst-sft-7-llama-30b' 'palm-2-chat-bison-001'\n",
            " 'rwkv-4-raven-14b' 'stablelm-tuned-alpha-7b' 'tulu-30b' 'vicuna-13b-v1.3'\n",
            " 'vicuna-33b-v1.3' 'vicuna-7b-v1.3' 'wizardlm-13b' 'wizardlm-30b'\n",
            " 'Llama-2-7b-chat' 'Llama-2-13b-chat' 'Llama-2-70b-chat']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.32.0.min.js\"></script>                <div id=\"1944259a-fc3f-48a6-b4d9-16cfe67de2e8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1944259a-fc3f-48a6-b4d9-16cfe67de2e8\")) {                    Plotly.newPlot(                        \"1944259a-fc3f-48a6-b4d9-16cfe67de2e8\",                        [{\"hovertemplate\":\"model=LLaMA-13B\\u003cbr\\u003escore=%{r}\\u003cbr\\u003ecategory=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"LLaMA-13B\",\"line\":{\"color\":\"rgb(102, 197, 204)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"LLaMA-13B\",\"r\":[3.7,4.05,2.55,1.05,1.1,2.5,2.55,3.35,3.7],\"showlegend\":true,\"subplot\":\"polar\",\"theta\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\",\"Writing\"],\"type\":\"scatterpolar\"},{\"hovertemplate\":\"model=GPT-4\\u003cbr\\u003escore=%{r}\\u003cbr\\u003ecategory=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"GPT-4\",\"line\":{\"color\":\"rgb(246, 207, 113)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"GPT-4\",\"r\":[9.65,8.9,9.0,6.8,8.55,9.375,9.7,9.95,9.65],\"showlegend\":true,\"subplot\":\"polar\",\"theta\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\",\"Writing\"],\"type\":\"scatterpolar\"},{\"hovertemplate\":\"model=Vicuna-13B-v.1.3\\u003cbr\\u003escore=%{r}\\u003cbr\\u003ecategory=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Vicuna-13B-v.1.3\",\"line\":{\"color\":\"rgb(248, 156, 116)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"Vicuna-13B-v.1.3\",\"r\":[9.25,7.175,5.85,2.6,3.25,5.55,7.975,9.45,9.25],\"showlegend\":true,\"subplot\":\"polar\",\"theta\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\",\"Writing\"],\"type\":\"scatterpolar\"},{\"hovertemplate\":\"model=GPT-3.5-turbo\\u003cbr\\u003escore=%{r}\\u003cbr\\u003ecategory=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"GPT-3.5-turbo\",\"line\":{\"color\":\"rgb(220, 176, 242)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"GPT-3.5-turbo\",\"r\":[9.2,8.4,5.65,6.3,6.9,8.85,8.7,9.55,9.2],\"showlegend\":true,\"subplot\":\"polar\",\"theta\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\",\"Writing\"],\"type\":\"scatterpolar\"},{\"hovertemplate\":\"model=Llama-2-70b-chat\\u003cbr\\u003escore=%{r}\\u003cbr\\u003ecategory=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Llama-2-70b-chat\",\"line\":{\"color\":\"rgb(135, 197, 95)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"Llama-2-70b-chat\",\"r\":[9.3,7.5,5.8,3.3,3.15,7.25,8.925,9.625,9.3],\"showlegend\":true,\"subplot\":\"polar\",\"theta\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\",\"Writing\"],\"type\":\"scatterpolar\"},{\"hovertemplate\":\"model=Llama-2-13b-chat\\u003cbr\\u003escore=%{r}\\u003cbr\\u003ecategory=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Llama-2-13b-chat\",\"line\":{\"color\":\"rgb(158, 185, 243)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"Llama-2-13b-chat\",\"r\":[8.85,7.5,5.1,3.45,3.0,6.925,8.625,9.75,8.85],\"showlegend\":true,\"subplot\":\"polar\",\"theta\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\",\"Writing\"],\"type\":\"scatterpolar\"},{\"hovertemplate\":\"model=Vicuna-33B-v1.3\\u003cbr\\u003escore=%{r}\\u003cbr\\u003ecategory=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Vicuna-33B-v1.3\",\"line\":{\"color\":\"rgb(254, 136, 177)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"Vicuna-33B-v1.3\",\"r\":[9.5,8.45,6.65,3.15,3.35,7.1,8.975,9.8,9.5],\"showlegend\":true,\"subplot\":\"polar\",\"theta\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\",\"Writing\"],\"type\":\"scatterpolar\"},{\"hovertemplate\":\"model=Llama-2-7b-chat\\u003cbr\\u003escore=%{r}\\u003cbr\\u003ecategory=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Llama-2-7b-chat\",\"line\":{\"color\":\"rgb(201, 219, 116)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"Llama-2-7b-chat\",\"r\":[8.9,7.7,4.25,2.4,3.0,6.5,8.65,8.75,8.9],\"showlegend\":true,\"subplot\":\"polar\",\"theta\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\",\"Writing\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"polar\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"angularaxis\":{\"direction\":\"clockwise\",\"rotation\":90,\"categoryorder\":\"array\",\"categoryarray\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\"]}},\"legend\":{\"title\":{\"text\":\"model\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1944259a-fc3f-48a6-b4d9-16cfe67de2e8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Disclaimer: Radar plot from the notebook: https://colab.research.google.com/drive/15O3Y8Rxq37PuMlArE291P4OC6ia37PQK#scrollTo=5i8R0l-XqkgO\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "CATEGORIES = [\"Writing\", \"Roleplay\", \"Reasoning\", \"Math\", \"Coding\", \"Extraction\", \"STEM\", \"Humanities\"]\n",
        "\n",
        "\n",
        "def get_model_df():\n",
        "    cnt = 0\n",
        "    q2result = []\n",
        "    fin = open(\"gpt-4_single.jsonl\", \"r\")\n",
        "    for line in fin:\n",
        "        obj = json.loads(line)\n",
        "        obj[\"category\"] = CATEGORIES[(obj[\"question_id\"]-81)//10]\n",
        "        q2result.append(obj)\n",
        "    df = pd.DataFrame(q2result)\n",
        "    return df\n",
        "\n",
        "def toggle(res_str):\n",
        "    if res_str == \"win\":\n",
        "        return \"loss\"\n",
        "    elif res_str == \"loss\":\n",
        "        return \"win\"\n",
        "    return \"tie\"\n",
        "\n",
        "def get_model_df_pair():\n",
        "    fin = open(\"gpt-4_pair.jsonl\", \"r\")\n",
        "    cnt = 0\n",
        "    q2result = []\n",
        "    for line in fin:\n",
        "        obj = json.loads(line)\n",
        "\n",
        "        result = {}\n",
        "        result[\"qid\"] = str(obj[\"question_id\"])\n",
        "        result[\"turn\"] = str(obj[\"turn\"])\n",
        "        if obj[\"g1_winner\"] == \"model_1\" and obj[\"g2_winner\"] == \"model_1\":\n",
        "            result[\"result\"] = \"win\"\n",
        "        elif obj[\"g1_winner\"] == \"model_2\" and obj[\"g2_winner\"] == \"model_2\":\n",
        "            result[\"result\"] = \"loss\"\n",
        "        else:\n",
        "            result[\"result\"] = \"tie\"\n",
        "        result[\"category\"] = CATEGORIES[(obj[\"question_id\"]-81)//10]\n",
        "        result[\"model\"] = obj[\"model_1\"]\n",
        "        q2result.append(result)\n",
        "\n",
        "    df = pd.DataFrame(q2result)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = get_model_df()\n",
        "df_pair = get_model_df_pair()\n",
        "\n",
        "all_models = df[\"model\"].unique()\n",
        "print(all_models)\n",
        "scores_all = []\n",
        "for model in all_models:\n",
        "    for cat in CATEGORIES:\n",
        "        # filter category/model, and score format error (<1% case)\n",
        "        res = df[(df[\"category\"]==cat) & (df[\"model\"]==model) & (df[\"score\"] >= 0)]\n",
        "        score = res[\"score\"].mean()\n",
        "        scores_all.append({\"model\": model, \"category\": cat, \"score\": score})\n",
        "\n",
        "target_models = [\"Llama-2-7b-chat\", \"vicuna-33b-v1.3\", \"Llama-2-13b-chat\", \"Llama-2-70b-chat\", \"gpt-3.5-turbo\",  \"vicuna-13b-v1.3\", \"gpt-4\", \"llama-13b\"]\n",
        "\n",
        "scores_target = [scores_all[i] for i in range(len(scores_all)) if scores_all[i][\"model\"] in target_models]\n",
        "\n",
        "# sort by target_models\n",
        "scores_target = sorted(scores_target, key=lambda x: target_models.index(x[\"model\"]), reverse=True)\n",
        "\n",
        "df_score = pd.DataFrame(scores_target)\n",
        "df_score = df_score[df_score[\"model\"].isin(target_models)]\n",
        "\n",
        "rename_map = {\"llama-13b\": \"LLaMA-13B\",\n",
        "              \"alpaca-13b\": \"Alpaca-13B\",\n",
        "              \"vicuna-33b-v1.3\": \"Vicuna-33B-v1.3\",\n",
        "              \"vicuna-13b-v1.3\": \"Vicuna-13B-v.1.3\",\n",
        "              \"gpt-3.5-turbo\": \"GPT-3.5-turbo\",\n",
        "              \"gpt-4\": \"GPT-4\"}\n",
        "\n",
        "for k, v in rename_map.items():\n",
        "    df_score.replace(k, v, inplace=True)\n",
        "\n",
        "fig = px.line_polar(df_score, r = 'score', theta = 'category', line_close = True, category_orders = {\"category\": CATEGORIES},\n",
        "                    color = 'model', markers=True, color_discrete_sequence=px.colors.qualitative.Pastel)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7iHH7Jse5cP"
      },
      "source": [
        "# Exercise #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoytRytTFO2w",
        "outputId": "a2c984ba-33d8-42b4-9ed9-01dc36c57952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# import packages\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# define computational device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Device: {device}\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(f\"Device: {device}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-410m\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-410m\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import set_seed  # reproducibility\n",
        "torch.manual_seed(42)\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyNOJOxRFO2y"
      },
      "outputs": [],
      "source": [
        "test_set = [\n",
        "    \"Input: A person on a horse jumps over a broken down airplane. A person is training his horse for a competition. Relation:\", # neutral\n",
        "    \"Input: A person on a horse jumps over a broken down airplane. A person is outdoors, on a horse. Relation:\",  # entailment\n",
        "    \"Input: Children smiling and waving at camera. There are children present. Relation:\",  # entailment\n",
        "    \"Input: A boy is jumping on skateboard in the middle of a red bridge. The boy skates down the sidewalk. Relation:\", # contradiction\n",
        "    \"Input: An older man sits with his orange juice at a small table in a coffee shop while employees in bright colored shirts smile in the background. An older man drinks his juice as he waits for his daughter to get off work. Relation:\", # neutral\n",
        "    \"Input: High fashion ladies wait outside a tram beside a crowd of people in the city. The women do not care what clothes they wear. Relation:\" # contradiction\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5AiSG45FO2y"
      },
      "outputs": [],
      "source": [
        "def pretty_print(s, text = None):\n",
        "    decoded = tokenizer.decode(s, skip_special_tokens=True)\n",
        "    if text is not None:\n",
        "        decoded = decoded.replace(text, \"\")\n",
        "    print(decoded)\n",
        "    print(100 * '-' + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8tEMwaKFO2y"
      },
      "outputs": [],
      "source": [
        "# greedy decoding\n",
        "def greedy_decoding(input_ids, max_new_tokens=2):\n",
        "    output = model.generate(input_ids, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
        "    return output\n",
        "\n",
        "def beam_search_decoding(input_ids, max_new_tokens=2, num_beams=5):\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=True,   # option `early_stopping` implies stopping when all beams reach the end-of-sentence token\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return output\n",
        "\n",
        "def pure_sampling_decoding(input_ids, max_new_tokens=2):\n",
        "    # # activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_k=0,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return output\n",
        "\n",
        "def softmax_sampling_decoding(input_ids, max_new_tokens=2, temperature=0.7):\n",
        "    # # activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_k=0,\n",
        "        temperature=temperature,  # higher temperature means more randomness\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return output\n",
        "\n",
        "def top_k_sampling_decoding(input_ids, max_new_tokens=2, k=50):\n",
        "    # # activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_k=k,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return output\n",
        "\n",
        "# the set of most likely words the summed probability of which exceeds threshold p  (also called nucleus sampling)\n",
        "def top_p_sampling_decoding(input_ids, max_new_tokens=2, p=0.9):\n",
        "    # # activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_p=p,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return output\n",
        "\n",
        "def contrastive_decoding(input_ids, max_new_tokens=2, penalty_alpha=0.6, top_k=4):\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        penalty_alpha=penalty_alpha,\n",
        "        top_k=top_k,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIF8LfGcFO2z"
      },
      "source": [
        "## Experiment Scheme\n",
        "In our experiments we worked on the following configurations:\n",
        "* Two models, `pythia-410m` and `pythia-1.4b` are used to generate the output.\n",
        "\n",
        "\n",
        "### Natural Language Inference\n",
        "\n",
        "* We use \"Instruction Prompting\" to make LM better understand user intention and follow the instruction.\n",
        "* Prefix of the input is given as follows:\\\n",
        " \"Please classify whether two sentences form a “contradiction” or an “entailment”, or the relation is “neutral”.\"\n",
        "* For each model, we test the following prompt engineering techniques:\n",
        "    * $k$-shot learning\n",
        "        * Zero-shot learning (no example is provided)\n",
        "        * One-shot learning (only one example is provided)\n",
        "        * Few-shot learning ($k$ = 3) (1 example for each relation)\n",
        "        * Few-shot learning ($k$ = 9) (3 examples for each relation)\n",
        "    * Self-consistency prompting (softmax sampling) with majority vote\n",
        "* For each $k$-shot learning scenario, we test the following decoding strategies:\n",
        "    * Greedy decoding\n",
        "    * Pure sampling\n",
        "    * Softmax sampling (temperature = 0.7)\n",
        "    * Top-$k$ sampling ($k$ = 50)\n",
        "    * Top-$p$ sampling ($p$ = 0.9)\n",
        "    * Beam search (beam size = 5)\n",
        "    * Contrastive decoding (penalty=0.6, $k$=4)\n",
        "\n",
        "In self-consistency, we apply majority vote on the outputs generate using softmax sampling.\n",
        "\n",
        "Disclaimer: As all these methods have different hyperparameters to tune, therefore the comparison is not completely fair. However, we believe that this comparison can give us a general idea of the performance of these methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAt_dT4TFO2z"
      },
      "source": [
        "## Natural Language Inference\n",
        "Natural language inference: the task is to classify whether two sentences form a “contradiction” or an “entailment”, or the relation is “neutral”.\n",
        "* A person on a horse jumps over a broken down airplane. A person is training his horse for a competition. **neutral**\n",
        "* A person on a horse jumps over a broken down airplane. A person is outdoors, on a horse. **entailment**\n",
        "* Children smiling and waving at camera. There are children present. **entailment**\n",
        "* A boy is jumping on skateboard in the middle of a red bridge. The boy skates down the sidewalk. **contradiction**\n",
        "* An older man sits with his orange juice at a small table in a coffee shop while employees in bright colored shirts smile in the background. An older man drinks his juice as he waits for his daughter to get off work. **neutral**\n",
        "* High fashion ladies wait outside a tram beside a crowd of people in the city. The women do not care what clothes they wear. **contradiction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hikkh-oaFO20"
      },
      "source": [
        "Few-shot examples are collected from the Stanford Natural Language Inference (SNLI) Corpus. \\\n",
        "Paper: https://arxiv.org/pdf/1508.05326\n",
        "\n",
        "We used two sets of few-shot examples to experiment on. First set consists of entirely independent sentences, while the second set consists of sentences that are related to each other. In the second set, three different \"hypothesis\" sentences are created for each \"premise\" sentence. The idea is to prevent the model from relying solely on the first sentence for the relation prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbapyTSXJyig"
      },
      "source": [
        "\n",
        "| Model | Prompt Engineering | Few-Shot Examples | Best Accuracy | Majority Vote Accuracy |\n",
        "| :---: | :---: | :---: |:---: |:---: |\n",
        "| pythia-410m | Zero-shot learning | -  | 0/6 | 0/6|\n",
        "| pythia-410m | One-shot learning |  - | 2/6 | 2/6 |\n",
        "| pythia-410m | Few-shot learning ($k = 3$) |  Independent | 3/6 | 3/6|\n",
        "| pythia-410m | Few-shot learning ($k = 9$) | Independent | 3/6 | 2/6 |\n",
        "| pythia-410m | Few-shot learning ($k = 3$) |Related | 3/6| 2/6 |\n",
        "| pythia-410m | Few-shot learning ($k = 9$) |  Related | 3/6 | 2/6 |\n",
        "| pythia-410m | Self-consistency  ($k = 3$) |  Independent | 2/6 | - |\n",
        "| pythia-410m | Self-consistency  ($k = 9$) |  Related | 2/6 | - |\n",
        "| pythia-1.4b | Zero-shot learning | -  | 0/6 | 0/6|\n",
        "| pythia-1.4b | One-shot learning |  - | 2/6  | 2/6|\n",
        "| pythia-1.4b | Few-shot learning ($k = 3$) |  Independent | 3/6 | 2/6|\n",
        "| pythia-1.4b | Few-shot learning ($k = 9$) | Independent | **4/6** (softmax sampling) | 2/6 |\n",
        "| pythia-1.4b | Few-shot learning ($k = 3$) |Related | **4/6** (contrastive decoding) | 3/6 |\n",
        "| pythia-1.4b | Few-shot learning ($k = 9$) |  Related | 3/6 | 2/6 |\n",
        "| pythia-1.4b | Self-consistency  ($k = 3$) |  Independent | 2/6 | - |\n",
        "| pythia-1.4b | Self-consistency  ($k = 9$) |  Related | 2/6 | - |\n",
        "\n",
        "* Best Accuracy: For optimal accuracy, we generate outputs for each decoding scheme and select the one with the highest accuracy, disregarding the others.\n",
        "* Majority Vote Accuracy (few-shot learning): Accuracy is determined by considering all outputs from different decoding schemes for a given input, with the final output decided by majority vote."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmSYUpinJ0r4"
      },
      "source": [
        "Observations:\n",
        "* When only the input is provided without any example or CoT prompting, the model doesn't even seem to understand the task - although the task is explicitly stated in the prompt. It attempts to complete the sentence with irrelevant information.\n",
        "* We tested all these approaches with and without CoT prompts. In this particular case, using the given models and few-shot examples, there doesn't seem to be any difference between prompting for reasoning and not.\n",
        "* There doesn't appear to be a hierarchy among the different decoding strategies, as the leading one varies from experiment to experiment.\n",
        "* The model seems to achieve the same accuracy for both independent and related few-shot examples.\n",
        "* `pythia-1.4b` appears to outperform `pythia-410m` when the few-shot examples are provided. However, the test set needs to be larger to draw a solid conclusion. Best accuracies are achieved when $k=3$ for both of the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqCv0gvNFO20"
      },
      "outputs": [],
      "source": [
        "# independent\n",
        "few_shot_examples_v1 = [\n",
        "    \"Input: Children smiling and waving at camera. There are children present. Relation: entailment\",\n",
        "    \"Input: A man inspects the uniform of a figure in some East Asian country. The man is sleeping. Relation: contradiction\", # contradiction\n",
        "    \"Input: An older and younger man smiling. Two men are smiling and laughing at the cats playing on the floor. Relation: neutral\", # neutral\n",
        "    \"Input: This church choir sings to the masses as they sing joyous songs from the book at a church. The church is filled with song. Relation: entailment\",  # entailment\n",
        "    \"Input: A black race car starts up in front of a crowd of people. A man is driving down a lonely road. Relation: contradiction\", # contradiction\n",
        "    \"Input: A smiling costumed woman is holding an umbrella. A happy woman in a fairy costume holds an umbrella. Relation: neutral\", # neutral\n",
        "    \"Input: A soccer game with multiple males playing. Some men are playing a sport. Relation: entailment\",  # entailment\n",
        "    \"Input: Four dirty and barefooted children. Four kids won awards for 'cleanest feet'. Relation: contradiction\", # contradiction\n",
        "    \"Input: A woman with a green headscarf, blue shirt and a very big grin.\tThe woman is young. Relation: neutral\", # neutral\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qHkbeLlFO20"
      },
      "outputs": [],
      "source": [
        "# related\n",
        "few_shot_examples_v2 = [\n",
        "    \"Input: Children smiling and waving at camera. There are children present. Relation: entailment\",  # entailment\n",
        "    \"Input: Children smiling and waving at camera. The kids are frowning. Relation: contradiction\", # contradiction\n",
        "    \"Input: Children smiling and waving at camera. They are smiling at their parents. Relation: neutral\", # neutral\n",
        "    \"Input: This church choir sings to the masses as they sing joyous songs from the book at a church. The church is filled with song. Relation: entailment\",  # entailment\n",
        "    \"Input: This church choir sings to the masses as they sing joyous songs from the book at a church. A choir singing at a baseball game. Relation: contradiction\", # contradiction\n",
        "    \"Input: This church choir sings to the masses as they sing joyous songs from the book at a church. The church has cracks in the ceiling. Relation: neutral\", # neutral\n",
        "    \"Input: A woman with a green headscarf, blue shirt and a very big grin.\tThe woman is very happy. Relation: entailment\", # entailment\n",
        "    \"Input: A woman with a green headscarf, blue shirt and a very big grin.\tThe woman has been shot. Relation: contradiction\", # contradiction\n",
        "    \"Input: A woman with a green headscarf, blue shirt and a very big grin.\tThe woman is young. Relation: neutral\", # neutral\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA5u77z_FO20"
      },
      "outputs": [],
      "source": [
        "def experiment(k, decoding, few_shot_examples, test_set):\n",
        "    print(f\"Decoding scheme: {decoding.__name__}\\n\")\n",
        "    for idx, test in enumerate(test_set):\n",
        "        suffix = \"\"\n",
        "        if(k):\n",
        "            few_shot = few_shot_examples[:k]\n",
        "            suffix = \"\\n\".join(few_shot) + \"\\n\"\n",
        "        task = \"Please classify whether two sentences form a “contradiction” or an “entailment”, or the relation is “neutral”. \\n\"\n",
        "        input_text = task + suffix +  test\n",
        "\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "        output = decoding(input_ids)\n",
        "        print(f\"{idx}) \" + test)\n",
        "        pretty_print(output[0], text=input_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ73fg-rJIte"
      },
      "outputs": [],
      "source": [
        "def majority_vote(k, few_shot_examples, test_set):\n",
        "  for idx, test in enumerate(test_set):\n",
        "    suffix = \"\"\n",
        "    if(k):\n",
        "        few_shot = few_shot_examples[:k]\n",
        "        suffix = \"\\n\".join(few_shot) + \"\\n\"\n",
        "    task = \"Please classify whether two sentences form a “contradiction” or an “entailment”, or the relation is “neutral”. \\n\"\n",
        "    input_text = task + suffix +  test\n",
        "\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    print(f\"{idx}) \" + test)\n",
        "    for decoding in [greedy_decoding, beam_search_decoding, pure_sampling_decoding, softmax_sampling_decoding, top_k_sampling_decoding, top_p_sampling_decoding, contrastive_decoding]:\n",
        "      output = decoding(input_ids)\n",
        "      decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "      decoded = decoded.replace(input_text, \"\").replace(\"\\n\",\"\")\n",
        "      print(decoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvjJMxsuFO21"
      },
      "outputs": [],
      "source": [
        "# decoding strategies: greedy, beam search, pure sampling, softmax sampling, top-k sampling, top-p sampling, contrastive decoding\n",
        "for decoding in [greedy_decoding, beam_search_decoding, pure_sampling_decoding, softmax_sampling_decoding, top_k_sampling_decoding, top_p_sampling_decoding, contrastive_decoding]:\n",
        "    experiment(3, decoding, few_shot_examples_v2, test_set)\n",
        "\n",
        "## GT:  neutral entailment entailment contradiction neutral contradiction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqZEe0gGWkXZ"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lDKJL9lLN4R"
      },
      "outputs": [],
      "source": [
        "majority_vote(1, few_shot_examples_v1, test_set)\n",
        "## GT:  neutral entailment entailment contradiction neutral contradiction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76g1C46N44YX"
      },
      "outputs": [],
      "source": [
        "def self_consistency(k, decoding, few_shot_examples, test_set, n = 5):\n",
        "  for idx, test in enumerate(test_set):\n",
        "    suffix = \"\"\n",
        "    if(k):\n",
        "        few_shot = few_shot_examples[:k]\n",
        "        suffix = \"\\n\".join(few_shot) + \"\\n\"\n",
        "    task = \"Please classify whether two sentences form a “contradiction” or an “entailment”, or the relation is “neutral”.\\n\"\n",
        "    input_text = task + suffix +  test\n",
        "\n",
        "\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    print(f\"{idx}) \" + test)\n",
        "\n",
        "    for i in range(n):\n",
        "      output = decoding(input_ids)\n",
        "      decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "      decoded = decoded.replace(input_text, \"\").replace(\"\\n\",\"\")\n",
        "      print(decoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXj5IauJ4dov"
      },
      "outputs": [],
      "source": [
        "self_consistency(9, softmax_sampling_decoding, few_shot_examples_v1, test_set)\n",
        "## GT:  neutral entailment entailment contradiction neutral contradiction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebz1LHG5AOxY"
      },
      "source": [
        "## Multiple-choice QA\n",
        "\n",
        "Multiple-choice QA: the task is to predict the correct answer option for the question, given the question and the options.\n",
        "\n",
        "\n",
        "## Experiment Scheme\n",
        "In our experiments we worked on the following configurations:\n",
        "* Two models, `pythia-410m` and `pythia-1.4b` are used to generate the output.\n",
        "* Few-shot examples are collected from two different datasets: [CommonSenseQA](https://huggingface.co/datasets/tau/commonsense_qa) and [NumerSense](https://inklab.usc.edu/NumerSense/) dataset. CommonsenseQA is a multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers. NumerSense is a numerical commonsense reasoning probing task, with a diagnostic dataset consisting of 3,145 masked-word-prediction probes.\n",
        "* We primarily explored two prompting techniques: Generated Knowledge Prompting (with few-shot examples) and Few-shot Prompting. In Generated Knowledge Prompting, the approach involves generating knowledge statements pertaining to the given question. Subsequently, we evaluate the log probability of each answer option being generated given these knowledge statements. This methodology enables the model to either support or oppose its own answer. We also employ two output extraction techniques: scoring the answers based on log-probabilities and classical generation. Classical generation utilizes the log-probabilities to generate new tokens. Hence, these approaches initially appear similar. However, the former allows us to constrain the model's output to only one of the options, thereby limiting its output possibilities. We worked on the following combinations:\n",
        "  * Generated Knowledge Prompting with NumerSense dataset + scoring\n",
        "  * Generated Knowledge Prompting with CommonSenseQA dataset + scoring\n",
        "  * Few Shot Learning with CommonSenseQA dataset + scoring\n",
        "  * Few Shot Learning with CommonSenseQA dataset + classical generation (softmax sampling)\n",
        "\n",
        "For the generated knowledge prompting, we generated five knowledge statements for each question. For each answer choice 𝑎, we identified the knowledge statement that best supports it by calculating the log probability of generating that answer for each augmented prompt (with the knowledge statement). This process allows us to determine the maximum probability for each answer. Ultimately, we select the answer with the highest maximum probability.\n",
        "\n",
        "All the few-shot prompts can be found below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqaVF_b4hfsF"
      },
      "source": [
        "| Model | Prompt Engineering | Dataset | Generation Technique | Accuracy |\n",
        "| :---: | :---: | :---: |:---: |:---: |\n",
        "| pythia-410m | Generated Knowledge Prompting | NumerSense  |  Log-Probability scoring | 3/6 |\n",
        "| pythia-410m | Generated Knowledge Prompting | CommonSenseQA  |  Log-Probability scoring | **4/6** |\n",
        "| pythia-410m | Few Shot learning | CommonSenseQA  |  Log-Probability scoring | 0/6 |\n",
        "| pythia-410m | Few Shot learning | CommonSenseQA  |  Generation with Softmax Sampling | 1/6 |\n",
        "| pythia-1.4b | Generated Knowledge Prompting | NumerSense  |  Log-Probability scoring | 2/6 |\n",
        "| pythia-1.4b| Generated Knowledge Prompting | CommonSenseQA  |  Log-Probability scoring | 2/6 |\n",
        "| pythia-1.4b | Few Shot learning | CommonSenseQA  |  Log-Probability scoring | 1/6 |\n",
        "| pythia-1.4b | Few Shot learning | CommonSenseQA  |  Generation with Softmax Sampling | 2/6 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkqDeAlVLi0Q"
      },
      "source": [
        "Observations:\n",
        "* Overall, generated knowledge prompting seems to outperform vanilla few shot learning (no knowledge statement is generated).\n",
        "* Maximum accuracy is achieved using generated knowledge prompting on CommonSenseQA with `pythia-410m`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtpskPrJWi9p"
      },
      "source": [
        "### Generated Knowledge Prompting with NumerSense Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzb1WL1-BxPg"
      },
      "outputs": [],
      "source": [
        "qa = {\n",
        "    \"The only baggage the woman checked was a drawstring bag, where was she heading with it?\": [\"garbage can\", \"military\", \"jewelry store\", \"safe\", \"airport\"],\n",
        "    \"To prevent any glare during the big football game he made sure to clean the dust of his what?\": [\"television\", \"attic\", \"corner\", \"they cannot clean corner and library during football match they cannot need that\", \"ground\"],\n",
        "    \"The president is the leader of what institution?\": [\"walmart\", \"white house\", \"country\", \"corporation\", \"government\"],\n",
        "    \"What kind of driving leads to accidents?\": [\"stressful\", \"dangerous\", \"fun\", \"illegal\", \"deadly\"],\n",
        "    \"Can you name a good reason for attending school?\": [\"get smart\", \"boredom\", \"colds and flu\", \"taking tests\", \"spend time\"],\n",
        "    \"Stanley had a dream that was very vivid and scary. He had trouble telling it from what?\": [\"imagination\", \"reality\", \"dreamworker\", \"nightmare\", \"awake\"]\n",
        "}\n",
        "\n",
        "# GT: airport - television - country - dangerous - get smart - reality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qKTFirwCpyJ",
        "outputId": "0368e6c8-3bcb-4485-c498-4e68a42c9fa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constructed few shot prompt\n",
            "How many wings do penguins have? We know that birds have two wings. penguin is a kind of bird.\n",
            "How many sides does a parallelogram have? We know that a rectangular is a parallelogram. a square is a parallelogram.\n",
            "What is the number of limbs a typical human being has? We know that human beings have four limbs.\n",
            "How many feet are there in a yard? We know that a yard is three feet.\n"
          ]
        }
      ],
      "source": [
        "# Run this to get NumerSense dataset few shot prompts\n",
        "import pandas as pd\n",
        "\n",
        "inputs = [\n",
        "    \"How many wings do penguins have?\",\n",
        "    \"How many sides does a parallelogram have?\",\n",
        "    \"What is the number of limbs a typical human being has?\",\n",
        "    \"How many feet are there in a yard?\",\n",
        "]\n",
        "\n",
        "knowledges = [\n",
        "    \"Birds have two wings. Penguin is a kind of bird.\",\n",
        "    \"A rectangular is a parallelogram. A square is a parallelogram.\",\n",
        "    \"Human beings have four limbs.\",\n",
        "    \"A yard is three feet.\",\n",
        "]\n",
        "\n",
        "# Creating the dataframe\n",
        "df = pd.DataFrame({\n",
        "    'input': inputs,\n",
        "    'knowledge': knowledges\n",
        "})\n",
        "df\n",
        "\n",
        "\n",
        "few_shot_template = \"\"\"{q} We know that {k}\"\"\"\n",
        "\n",
        "few_shot_prompt = \"\\n\".join([\n",
        "    few_shot_template.format(\n",
        "        q=df.loc[i, \"input\"],\n",
        "        k=df.loc[i, \"knowledge\"].lower()\n",
        "    )\n",
        "    for i in range(len(df))\n",
        "])\n",
        "print(\"Constructed few shot prompt\\n\" + few_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDRs7hJpkxq1",
        "outputId": "8445879b-000d-49e8-cd6f-0d128ddbdb28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constructed few shot prompt\n",
            "Google Maps and other highway and street GPS services have replaced what? We know that electronic maps are the modern version of paper atlas.\n",
            "The fox walked from the city into the forest, what was it looking for? We know that natural habitats are usually away from cities.\n",
            "You can share files with someone if you have a connection to a what? We know that files can be shared over the internet.\n",
            "Too many people want exotic snakes. The demand is driving what to carry them? We know that some people raise snakes as pets.\n",
            "The bodyguard was good at his duties, he made the person who hired him what? We know that the job of bodyguards is to ensure the safety and security of the employer.\n"
          ]
        }
      ],
      "source": [
        "# Run this to get CommonSenseQA dataset few shot prompts\n",
        "\n",
        "inputs = [\n",
        "    \"Google Maps and other highway and street GPS services have replaced what?\",\n",
        "    \"The fox walked from the city into the forest, what was it looking for?\",\n",
        "    \"You can share files with someone if you have a connection to a what?\",\n",
        "    \"Too many people want exotic snakes. The demand is driving what to carry them?\",\n",
        "    \"The bodyguard was good at his duties, he made the person who hired him what?\"\n",
        "]\n",
        "\n",
        "knowledges = [\n",
        "    \"Electronic maps are the modern version of paper atlas.\",\n",
        "    \"Natural habitats are usually away from cities.\",\n",
        "    \"Files can be shared over the Internet.\",\n",
        "    \"Some people raise snakes as pets.\",\n",
        "    \"The job of bodyguards is to ensure the safety and security of the employer.\"\n",
        "]\n",
        "# Creating the dataframe\n",
        "df = pd.DataFrame({\n",
        "    'input': inputs,\n",
        "    'knowledge': knowledges\n",
        "})\n",
        "df\n",
        "\n",
        "\n",
        "few_shot_template = \"\"\"{q} We know that {k}\"\"\"\n",
        "\n",
        "few_shot_prompt = \"\\n\".join([\n",
        "    few_shot_template.format(\n",
        "        q=df.loc[i, \"input\"],\n",
        "        k=df.loc[i, \"knowledge\"].lower()\n",
        "    )\n",
        "    for i in range(len(df))\n",
        "])\n",
        "print(\"Constructed few shot prompt\\n\" + few_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2o1aqYCZryI",
        "outputId": "a3f85c10-d544-47f1-f21e-feb0a5007d51"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "bags are usually carried by the person carrying them.\n",
            "\n",
            "So,\n"
          ]
        }
      ],
      "source": [
        "# Only for visualizing a generated knowledge:\n",
        "question = list(qa)[0]\n",
        "choices = qa[question]\n",
        "prompt_input_ids = tokenizer(\n",
        "    few_shot_prompt + \"\\n\" + question + \" We know that \",\n",
        "    return_tensors=\"pt\"\n",
        ").input_ids.to(device)\n",
        "\n",
        "knowledge_statements = model.generate(\n",
        "    prompt_input_ids,\n",
        "    max_new_tokens=15,\n",
        "    do_sample=True,\n",
        "    temperature=0.5\n",
        ")\n",
        "# access the knowledge statements (i.e., only text that comes after prompt)\n",
        "knowledge = tokenizer.decode(\n",
        "    knowledge_statements[0, prompt_input_ids.shape[-1]:],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "print(\"Generated knowledge: \", knowledge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLSgwRy5aETD",
        "outputId": "1bc489ea-4cbe-48cc-f4c7-29f70798bc82"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "the woman checked her luggage, but she didn't take any of her\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "baggage is usually checked in checked baggage.\n",
            "The man was not\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "women carry small bags, especially when they're on the go.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "the woman checked her bag before she left.\n",
            "What was the reason\n",
            "Generated knowledge:  \n",
            "bags are usually carried in a carrier bag or a briefcase.\n",
            "\n",
            "Answer  garbage can Answer probabilities for each knowledge statement:  [-8.651183128356934, -8.358133316040039, -9.2638578414917, -8.854644775390625, -8.918338775634766]\n",
            "Answer  military Answer probabilities for each knowledge statement:  [-13.746541976928711, -13.27782917022705, -14.510452270507812, -14.339978218078613, -14.297685623168945]\n",
            "Answer  jewelry store Answer probabilities for each knowledge statement:  [-9.776968002319336, -10.095916748046875, -10.659232139587402, -9.95555591583252, -10.486684799194336]\n",
            "Answer  safe Answer probabilities for each knowledge statement:  [-11.570777893066406, -11.832415580749512, -13.639629364013672, -12.748946189880371, -13.790425300598145]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer  airport Answer probabilities for each knowledge statement:  [-11.538839340209961, -9.888652801513672, -12.38808536529541, -11.434816360473633, -11.631083488464355]\n",
            "Selected answer  garbage can with log P  -8.358133316040039\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  umpires are also responsible for cleaning the field.\n",
            "The police officer was\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  icing is used to stop glare on the ice.\n",
            "The computer software was\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  icing is a technique used to prevent glare in the game of football.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  umpires are responsible for preventing any glare.\n",
            "The person who wanted to\n",
            "Generated knowledge:  umpires are the workers who make sure the game goes well.\n",
            "You\n",
            "Answer  television Answer probabilities for each knowledge statement:  [-10.235928535461426, -9.407893180847168, -8.890667915344238, -9.370429039001465, -11.198206901550293]\n",
            "Answer  attic Answer probabilities for each knowledge statement:  [-13.596774101257324, -12.11440658569336, -11.61604118347168, -11.67751693725586, -13.281587600708008]\n",
            "Answer  corner Answer probabilities for each knowledge statement:  [-11.031961441040039, -10.703117370605469, -10.687712669372559, -11.021014213562012, -11.909016609191895]\n",
            "Answer  they cannot clean corner and library during football match they cannot need that Answer probabilities for each knowledge statement:  [-6.271496772766113, -6.655148983001709, -6.51792049407959, -6.4584479331970215, -6.452688217163086]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer  ground Answer probabilities for each knowledge statement:  [-8.554006576538086, -9.23339557647705, -8.114116668701172, -8.754240989685059, -9.520474433898926]\n",
            "Selected answer  they cannot clean corner and library during football match they cannot need that with log P  -6.271496772766113\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  umpires are the people who make the decisions about what to call an \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "presidents can be elected by the people.\n",
            "\n",
            "The president is\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "the president is the leader of what institution? We know that the president\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "the president has a lot of power, but the president is also the\n",
            "Generated knowledge:  umpires are the people who officiate at baseball games.\n",
            "The cat\n",
            "Answer  walmart Answer probabilities for each knowledge statement:  [-8.142420768737793, -8.972271919250488, -8.90455436706543, -8.03041934967041, -7.8204474449157715]\n",
            "Answer  white house Answer probabilities for each knowledge statement:  [-8.612419128417969, -7.63491153717041, -7.091569900512695, -6.645547866821289, -8.576825141906738]\n",
            "Answer  country Answer probabilities for each knowledge statement:  [-10.13504695892334, -12.369648933410645, -13.64605712890625, -11.518067359924316, -11.83149242401123]\n",
            "Answer  corporation Answer probabilities for each knowledge statement:  [-12.276028633117676, -14.06977367401123, -16.650859832763672, -13.893194198608398, -13.85256576538086]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer  government Answer probabilities for each knowledge statement:  [-10.173294067382812, -9.73061466217041, -11.749650001525879, -9.258166313171387, -10.929475784301758]\n",
            "Selected answer  white house with log P  -6.645547866821289\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "driving is a very dangerous job.\n",
            "What kind of car was the\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  ####### has a very high accident rate.\n",
            "What do you mean by\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "There are many types of accidents:\n",
            "\n",
            "Faulty brakes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "- the driver is under the influence of drugs\n",
            "- the driver is\n",
            "Generated knowledge:  \n",
            "accidents happen because of what? We know that we can’t\n",
            "Answer  stressful Answer probabilities for each knowledge statement:  [-14.708761215209961, -15.258934020996094, -15.46780014038086, -14.419212341308594, -14.7927885055542]\n",
            "Answer  dangerous Answer probabilities for each knowledge statement:  [-10.114299774169922, -11.092577934265137, -11.165120124816895, -9.094646453857422, -10.583547592163086]\n",
            "Answer  fun Answer probabilities for each knowledge statement:  [-14.501969337463379, -14.986560821533203, -16.233652114868164, -14.622293472290039, -13.889074325561523]\n",
            "Answer  illegal Answer probabilities for each knowledge statement:  [-11.216497421264648, -13.336479187011719, -12.792975425720215, -9.515718460083008, -11.635777473449707]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer  deadly Answer probabilities for each knowledge statement:  [-12.690098762512207, -14.63901424407959, -15.035849571228027, -12.710489273071289, -13.199512481689453]\n",
            "Selected answer  dangerous with log P  -9.094646453857422\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "school is expensive.\n",
            "\n",
            "A:\n",
            "\n",
            "There are several reasons\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "schools are meant to prepare students for the future.\n",
            "\n",
            "The\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "schools are for learning and education, they do not teach people how\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "schools will be the place for you to learn what? We know\n",
            "Generated knowledge:  ith the education system.\n",
            "\n",
            "A:\n",
            "\n",
            "I'm not sure\n",
            "Answer  get smart Answer probabilities for each knowledge statement:  [-8.589640617370605, -8.743338584899902, -8.013982772827148, -8.673873901367188, -9.343545913696289]\n",
            "Answer  boredom Answer probabilities for each knowledge statement:  [-7.858763694763184, -7.82045841217041, -7.263005256652832, -7.9718918800354, -7.668207168579102]\n",
            "Answer  colds and flu Answer probabilities for each knowledge statement:  [-5.3271989822387695, -5.516082286834717, -5.24860143661499, -5.370843410491943, -5.465941429138184]\n",
            "Answer  taking tests Answer probabilities for each knowledge statement:  [-8.29901123046875, -7.830539703369141, -7.338224411010742, -8.12620735168457, -7.884116172790527]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer  spend time Answer probabilities for each knowledge statement:  [-7.690305233001709, -7.893522262573242, -7.179699897766113, -7.780394554138184, -8.239646911621094]\n",
            "Selected answer  colds and flu with log P  -5.24860143661499\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "Stanley had trouble telling it from what? We know that Stanley had\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  umpires are often called to decide games.\n",
            "A person who is very\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "Stanley had a dream that was very vivid and scary. He had\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated knowledge:  \n",
            "Stanley had an aversion to heights.\n",
            "The man was a\n",
            "Generated knowledge:  icing is used in medicine to treat pain.\n",
            "The computer was a big\n",
            "Answer  imagination Answer probabilities for each knowledge statement:  [-14.361309051513672, -11.466972351074219, -12.520139694213867, -9.94018840789795, -11.04466724395752]\n",
            "Answer  reality Answer probabilities for each knowledge statement:  [-12.76948356628418, -9.612512588500977, -9.671875, -7.55483865737915, -8.412652015686035]\n",
            "Answer  dreamworker Answer probabilities for each knowledge statement:  [-14.165630340576172, -10.419055938720703, -12.055522918701172, -12.118217468261719, -11.015838623046875]\n",
            "Answer  nightmare Answer probabilities for each knowledge statement:  [-15.019416809082031, -11.555357933044434, -11.577406883239746, -10.60135555267334, -11.841059684753418]\n",
            "Answer  awake Answer probabilities for each knowledge statement:  [-15.815916061401367, -12.528425216674805, -13.598419189453125, -13.201595306396484, -13.424176216125488]\n",
            "Selected answer  reality with log P  -7.55483865737915\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# 3. Score each answer to the question based on the knowledge statements\n",
        "# as the score, we take the average log probability of the tokens in the answer\n",
        "# iterate over the answer options\n",
        "import numpy as np\n",
        "no_knowledge_statements = 5\n",
        "\n",
        "for question in list(qa):\n",
        "  answers = qa[question]\n",
        "  answer_log_probs = []\n",
        "  prompt_input_ids = tokenizer(\n",
        "    few_shot_prompt + \"\\n\" + question + \" We know that \",\n",
        "    return_tensors=\"pt\"\n",
        "    ).input_ids.to(device)\n",
        "\n",
        "  knowledge_statements = []\n",
        "  for i in range(no_knowledge_statements):\n",
        "    knowledge = model.generate(\n",
        "        prompt_input_ids,\n",
        "        max_new_tokens=15,\n",
        "        do_sample=True,\n",
        "        temperature=0.5\n",
        "    )\n",
        "\n",
        "    # access the knowledge statements (i.e., only text that comes after prompt)\n",
        "    knowledge = tokenizer.decode(\n",
        "        knowledge[0, prompt_input_ids.shape[-1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    knowledge_statements.append(knowledge)\n",
        "    print(\"Generated knowledge: \", knowledge)\n",
        "\n",
        "  # now we have knowledge statements in hand\n",
        "  maximizing_answer = None\n",
        "  maximizing_log_prob = -float(\"inf\")\n",
        "  for a in answers:\n",
        "    log_probs_for_a = []\n",
        "    for knowledge in knowledge_statements:\n",
        "        # construct the full prompt\n",
        "        prompt = f\"{knowledge} {question} {a}\"\n",
        "        # construct the prompt without the answer to create a mask which will\n",
        "        # allow to retrieve the token probabilities for tokens in the answer only\n",
        "        context_prompt = f\"{knowledge} {question}\"\n",
        "        # tokenize the prompt\n",
        "        input_ids = tokenizer(prompt,\n",
        "                            return_tensors=\"pt\").input_ids.to(device)\n",
        "        # tokenize the context prompt\n",
        "        context_input_ids = tokenizer(context_prompt,\n",
        "                                    return_tensors=\"pt\").input_ids\n",
        "        # create a mask with -100 for all tokens in the context prompt\n",
        "        # the -100 indicates that the token should be ignored in the loss computation\n",
        "        masked_labels = torch.ones_like(input_ids) * -100\n",
        "        masked_labels[:, context_input_ids.shape[-1]:] = input_ids[:, context_input_ids.shape[-1]:]\n",
        "        # generate the answer\n",
        "        preds = model(\n",
        "            input_ids,\n",
        "            labels=masked_labels\n",
        "        )\n",
        "        # retrieve the average log probability of the tokens in the answer\n",
        "        log_p = preds.loss.item()\n",
        "        log_probs_for_a.append(-log_p)\n",
        "    max_prob = np.max(log_probs_for_a)\n",
        "    if max_prob > maximizing_log_prob:\n",
        "        maximizing_log_prob = max_prob\n",
        "        maximizing_answer = a\n",
        "    print(\"Answer \", a, \"Answer probabilities for each knowledge statement: \", log_probs_for_a)\n",
        "  print(\"Selected answer \", maximizing_answer, \"with log P \", maximizing_log_prob)\n",
        "  print(100 * \"-\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cx11QSXWa_E"
      },
      "source": [
        "### Few-Shot Prompting with CommonSenseQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCuf5WvSN-7Y",
        "outputId": "74249cb9-8f5d-4157-9ee7-1b6bc38bf613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task: Predict the correct answer option for the question provided, considering the available options.\n",
            "Instructions: Review the question and choices provided, then select the option you believe is the correct answer.\n",
            "\n",
            "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
            "Options:\n",
            "A. bank\n",
            "B. library\n",
            "C. department store\n",
            "D. mall\n",
            "E. new york\n",
            "Selected Choice: A\n",
            "\n",
            "Question: What do people aim to do at work?\n",
            "Options:\n",
            "A. complete job\n",
            "B. learn from each other\n",
            "C. kill animals\n",
            "D. wear hats\n",
            "E. talk to each other\n",
            "Selected Choice: A\n",
            "\n",
            "Question: Where would you find magazines alongside many other printed works?\n",
            "Options:\n",
            "A. doctor\n",
            "B. bookstore\n",
            "C. market\n",
            "D. train station\n",
            "E. mortuary\n",
            "Selected Choice: B\n",
            "\n",
            "Question: Where are you likely to find a hamburger?\n",
            "Options:\n",
            "A. fast food restaurant\n",
            "B. pizza\n",
            "C. ground up dead cows\n",
            "D. mouth\n",
            "E. cow carcass\n",
            "Selected Choice: A\n",
            "\n",
            "Question: James was looking for a good place to buy farmland. Where might he look?\n",
            "Options:\n",
            "A. midwest\n",
            "B. countryside\n",
            "C. estate\n",
            "D. farming areas\n",
            "E. illinois\n",
            "Selected Choice: A\n",
            "\n",
            "Question: What island country is ferret popular?\n",
            "Options:\n",
            "A. own home\n",
            "B. north carolina\n",
            "C. great britain\n",
            "D. hutch\n",
            "E. outdoors\n",
            "Selected Choice: C\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = [\n",
        "    ('A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?', ['A. bank', 'B. library', 'C. department store', 'D. mall', 'E. new york'], 'A'),\n",
        "    ('What do people aim to do at work?', ['A. complete job', 'B. learn from each other', 'C. kill animals', 'D. wear hats', 'E. talk to each other'], 'A'),\n",
        "    ('Where would you find magazines alongside many other printed works?', ['A. doctor', 'B. bookstore', 'C. market', 'D. train station', 'E. mortuary'], 'B'),\n",
        "    ('Where are you likely to find a hamburger?', ['A. fast food restaurant', 'B. pizza', 'C. ground up dead cows', 'D. mouth', 'E. cow carcass'], 'A'),\n",
        "    ('James was looking for a good place to buy farmland. Where might he look?', ['A. midwest', 'B. countryside', 'C. estate', 'D. farming areas', 'E. illinois'], 'A'),\n",
        "    ('What island country is ferret popular?', ['A. own home', 'B. north carolina', 'C. great britain', 'D. hutch', 'E. outdoors'], 'C')\n",
        "]\n",
        "df = pd.DataFrame(data, columns=['Question', 'Answer_Options', 'Correct_Answer'])\n",
        "\n",
        "# Define task and instructions prompts\n",
        "task_prompt = \"Task: Predict the correct answer option for the question provided, considering the available options.\"\n",
        "instructions_prompt = \"Instructions: Review the question and choices provided, then select the option you believe is the correct answer.\"\n",
        "\n",
        "# Generate prompt with multiple examples\n",
        "few_shot_prompt = f\"{task_prompt}\\n{instructions_prompt}\\n\"\n",
        "for index, row in df.iterrows():\n",
        "    few_shot_prompt += \"\\n\"\n",
        "    few_shot_prompt += f\"Question: {row['Question']}\\nOptions:\\n\"\n",
        "    for option in row['Answer_Options']:\n",
        "        few_shot_prompt += f\"{option}\\n\"\n",
        "    few_shot_prompt += f\"Selected Choice: {row['Correct_Answer']}\\n\"\n",
        "\n",
        "# Display prompt\n",
        "print(few_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ67-v7xWgdC",
        "outputId": "38c8f069-dd28-4390-af0f-d808d4294333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task: Predict the correct answer option for the question provided, considering the available options.\n",
            "Instructions: Review the question and choices provided, then select the option you believe is the correct answer.\n",
            "\n",
            "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
            "Options:\n",
            "A. bank\n",
            "B. library\n",
            "C. department store\n",
            "D. mall\n",
            "E. new york\n",
            "Selected Choice: A\n",
            "\n",
            "Question: What do people aim to do at work?\n",
            "Options:\n",
            "A. complete job\n",
            "B. learn from each other\n",
            "C. kill animals\n",
            "D. wear hats\n",
            "E. talk to each other\n",
            "Selected Choice: A\n",
            "\n",
            "Question: Where would you find magazines alongside many other printed works?\n",
            "Options:\n",
            "A. doctor\n",
            "B. bookstore\n",
            "C. market\n",
            "D. train station\n",
            "E. mortuary\n",
            "Selected Choice: B\n",
            "\n",
            "Question: Where are you likely to find a hamburger?\n",
            "Options:\n",
            "A. fast food restaurant\n",
            "B. pizza\n",
            "C. ground up dead cows\n",
            "D. mouth\n",
            "E. cow carcass\n",
            "Selected Choice: A\n",
            "\n",
            "Question: James was looking for a good place to buy farmland. Where might he look?\n",
            "Options:\n",
            "A. midwest\n",
            "B. countryside\n",
            "C. estate\n",
            "D. farming areas\n",
            "E. illinois\n",
            "Selected Choice: A\n",
            "\n",
            "Question: What island country is ferret popular?\n",
            "Options:\n",
            "A. own home\n",
            "B. north carolina\n",
            "C. great britain\n",
            "D. hutch\n",
            "E. outdoors\n",
            "Selected Choice: C\n",
            "\n",
            "Question: The only baggage the woman checked was a drawstring bag, where was she heading with it?\n",
            "Options:\n",
            "A. garbage can\n",
            "B. military\n",
            "C. jewelry store\n",
            "D. safe\n",
            "E. airport\n",
            "Selected Choice:\n"
          ]
        }
      ],
      "source": [
        "question = list(qa)[0]\n",
        "choices = qa[question]\n",
        "\n",
        "print(few_shot_prompt + \"\\nQuestion: \" + question + \"\\nOptions:\\n\" + \"\\n\".join([chr(ord('A') + i) + \". \" + choices[i] for i in range(len(choices))]) + \"\\nSelected Choice:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ddGgg2aKWj",
        "outputId": "76d76315-a69e-49ab-ef1a-27bc8e912632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All answers  ['garbage can', 'military', 'jewelry store', 'safe', 'airport']\n",
            "Answer probabilities  [-1.149327039718628, -0.8839691281318665, -1.7674624919891357, -2.4614875316619873, -5.297186851501465]\n",
            "Selected answer  military with log P  -0.8839691281318665\n",
            "----------------------------------------------------------------------------------------------------\n",
            "All answers  ['television', 'attic', 'corner', 'they cannot clean corner and library during football match they cannot need that', 'ground']\n",
            "Answer probabilities  [-1.6051456928253174, -1.3155701160430908, -1.452256441116333, -1.5575783252716064, -2.7076313495635986]\n",
            "Selected answer  attic with log P  -1.3155701160430908\n",
            "----------------------------------------------------------------------------------------------------\n",
            "All answers  ['walmart', 'white house', 'country', 'corporation', 'government']\n",
            "Answer probabilities  [-1.0540814399719238, -0.9338192343711853, -1.921186923980713, -2.363598346710205, -4.736490726470947]\n",
            "Selected answer  white house with log P  -0.9338192343711853\n",
            "----------------------------------------------------------------------------------------------------\n",
            "All answers  ['stressful', 'dangerous', 'fun', 'illegal', 'deadly']\n",
            "Answer probabilities  [-0.7589095234870911, -1.0533545017242432, -2.1971399784088135, -2.8056986331939697, -6.040278911590576]\n",
            "Selected answer  stressful with log P  -0.7589095234870911\n",
            "----------------------------------------------------------------------------------------------------\n",
            "All answers  ['get smart', 'boredom', 'colds and flu', 'taking tests', 'spend time']\n",
            "Answer probabilities  [-1.03721284866333, -0.8900933861732483, -1.955305576324463, -2.6002087593078613, -4.708530902862549]\n",
            "Selected answer  boredom with log P  -0.8900933861732483\n",
            "----------------------------------------------------------------------------------------------------\n",
            "All answers  ['imagination', 'reality', 'dreamworker', 'nightmare', 'awake']\n",
            "Answer probabilities  [-0.9054934978485107, -1.1341121196746826, -1.681248426437378, -2.6236846446990967, -5.031479835510254]\n",
            "Selected answer  imagination with log P  -0.9054934978485107\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "answer_log_probs = []\n",
        "# iterate over the answer options\n",
        "# NOTE: This can take a moment\n",
        "\n",
        "for question in list(qa):\n",
        "  answers = qa[question]\n",
        "  answer_log_probs = []\n",
        "\n",
        "  for choice in ['A','B','C','D','E']:\n",
        "    # construct the full prompt\n",
        "    context_prompt = few_shot_prompt + \"\\nQuestion: \" + question + \"\\nOptions:\\n\" + \"\\n\".join([chr(ord('A') + i) + \". \" + answers[i] for i in range(len(answers))]) + \"\\nSelected Choice:\"\n",
        "    prompt = context_prompt + \" \" + choice\n",
        "\n",
        "    # construct the prompt without the answer to create a mask which will\n",
        "    # allow to retrieve the token probabilities for tokens in the answer only\n",
        "    # tokenize the prompt\n",
        "    input_ids = tokenizer(prompt,\n",
        "                          return_tensors=\"pt\").input_ids.to(device)\n",
        "    # tokenize the context prompt\n",
        "    context_input_ids = tokenizer(context_prompt,\n",
        "                                  return_tensors=\"pt\").input_ids\n",
        "    # create a mask with -100 for all tokens in the context prompt\n",
        "    # the -100 indicates that the token should be ignored in the loss computation\n",
        "    masked_labels = torch.ones_like(input_ids) * -100\n",
        "    masked_labels[:, context_input_ids.shape[-1]:] = input_ids[:, context_input_ids.shape[-1]:]\n",
        "    # generate the answer\n",
        "    preds = model(\n",
        "        input_ids,\n",
        "        labels=masked_labels\n",
        "    )\n",
        "    # retrieve the average log probability of the tokens in the answer\n",
        "    log_p = preds.loss.item()\n",
        "    answer_log_probs.append(-log_p)\n",
        "  import numpy as np\n",
        "  print(\"All answers \", answers)\n",
        "  print(\"Answer probabilities \", answer_log_probs)\n",
        "  max_prob_idx = np.argmax(answer_log_probs)\n",
        "  print(\"Selected answer \", answers[max_prob_idx], \"with log P \", answer_log_probs[max_prob_idx])\n",
        "  print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZajMy1tabxwv",
        "outputId": "172ca4ae-634b-4560-cd39-d5c86c81ee3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The only baggage the woman checked was a drawstring bag, where was she heading with it?\n",
            " B\n",
            "To prevent any glare during the big football game he made sure to clean the dust of his what?\n",
            " B\n",
            "The president is the leader of what institution?\n",
            " A\n",
            "What kind of driving leads to accidents?\n",
            " B\n",
            "Can you name a good reason for attending school?\n",
            " B\n",
            "Stanley had a dream that was very vivid and scary. He had trouble telling it from what?\n",
            " D\n"
          ]
        }
      ],
      "source": [
        "for question in list(qa):\n",
        "  answers = qa[question]\n",
        "  prompt = few_shot_prompt + \"\\nQuestion: \" + question + \"\\nOptions:\\n\" + \"\\n\".join([chr(ord('A') + i) + \". \" + answers[i] for i in range(len(answers))]) + \"\\nSelected Choice:\"\n",
        "\n",
        "  input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  output = softmax_sampling_decoding(input_ids)\n",
        "  decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "  decoded = decoded.replace(prompt, \"\").replace(\"\\n\",\"\")\n",
        "  print(question)\n",
        "  print(decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2WBBrn5dnsS"
      },
      "source": [
        "# Exercise #3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc17UTCNe5cV"
      },
      "source": [
        "**Q1**: How were words / tokens represented? What is the difference / similarity to modern LLMs?\n",
        "\n",
        "**A**: Bengio et al. propose a word-level model* where each word is mapped to a feature vector using an embedding matrix $C$ of size $|V| \\times m$, where $|V|$ is the vocabulary size and $m$ is the embedding dimension. While the exact mapping process isn't detailed, it likely involves one-hot encoding followed by matrix multiplication. This approach to word embeddings is also common in modern large language models (LLMs). For instance, in transformers, input words are mapped to embeddings using an embedding layer. These layers can be learnable or pretrained. A key difference in modern LLMs is that these embeddings are combined with positional encodings to capture the order of words in the sequence.\n",
        "\n",
        "Since Bengio et al.'s model operates at the word level, it is more prone to encountering out-of-vocabulary (OOV) words compared to modern LLMs. Modern LLMs handle OOV words using strategies like subword or character-level tokenization and fine-tuning the embedding layer. Bengio et al.'s model is more likely to encounter with OOV words in the test set. However, they claim that their model outperforms $n$-gram models in handling OOV words and offer a solution by using a weighted convex combination of the feature vectors of other words that could occur in the same context. A key difference is that Bengio et al. use static embeddings while modern LLMs can make use of contextualised embeddings.\n",
        "\n",
        "*: Rare words with frequency ≤ 3 were merged into a single symbol.\n",
        "\n",
        "---\n",
        "**Q2**: How was the context represented? What is the difference / similarity to modern LLMs?\n",
        "\n",
        "**A**: Similar to $n$-gram models, preceding $n$ words are considered as context in Bengio et al.'s model. However, since the model utilizes word embeddings this time (in contrast to $n$-gram models), it is reported to be able to take advantage of more context (on Brown, going from 2 words of context to 4 words brought improvements to the neural network, not to the $n$-grams). The idea of \"context window\" is still present in modern LLMs, but the context length is extremely longer. For instance, GPT-4 has two versions with context windows of 8,192 and 32,768 tokens*. Another difference is that, in modern LLMs, the context is not limited to the preceding words; it includes both preceding and following words. This is achieved through bi-directional transformers, introduced in models like BERT and GPT-2.\n",
        "\n",
        "*: https://en.wikipedia.org/wiki/GPT-4\n",
        "\n",
        "---\n",
        "**Q3**: What is the curse of dimensionality? Give a concrete example in the context of language modeling.\n",
        "\n",
        "**A**: _Curse of dimensionality_ refers to the problems that arise when working with high-dimensional data. Initially, this term described the phenomenon where algorithms that perform efficiently in low-dimensional spaces become intractable in high dimensions. As discussed in the article [A Few Useful Things to Know About Machine Learning](https://homes.cs.washington.edu/%7Epedrod/papers/cacm12.pdf), intuition fails in high dimensions. In the realm of higher-dimensional data analysis, classical distance metrics, such as the Euclidean distance, often exhibit limitations. For instance, a nearest neighbor classifier can become ineffective due to noise from irrelevant features or because all examples appear similar, making the classification essentially random.\n",
        "\n",
        "In the context of language modeling, as pointed out in the [paper](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf), modeling the joint probability distribution of a sentence becomes increasingly complex as the length of the sentence grows. If there are $n$ words in the sentence, and the vocabulary size is $|V|$, the number of possible sentences is $|V|^n$ - leading to $|V|^n - 1$ many free parameters (1 is reducted as the probabilities must sum up to 1 - making the last probability dependent on others). Modelling the joint distribution between many discrete random variables turns out to be intractable. Therefore we have to use another approach to model the language - that's why the paper proposes the distributed representations. The main idea is to do dimensionality reduction using word embeddings. Although the model is billions of parameters, representations consist of a few hundred dimensions (set as a hyperparameter). Embedding matrix consists of $|V| \\times m$ many entries where $m$ stands for the dimension. Therefore the number of free parameters scales **linearly** with $|V|$. Similarly the linear projections are used in transformers to reduce the dimensionality of the input. One can even say that the task of a fully connected layer is projecting the high-dimensional input to a lower-dimensional space so that the computations become intractable.\n",
        "\n",
        "---\n",
        "\n",
        "**Q4**: Which training data was used? What is the difference / similarity to modern LLMs?\n",
        "\n",
        "**A**: Two datasets are used for the experiments: Brown corpus which is a stream of 1,181,041 words, from a large variety of English texts and books, and the Associated Press (AP) News from 1995 and 1996 (a stream of about 14 million (13,994,528) words). From a very simplistic perspective, the data is similar to the ones used by modern LLMs - consisting of sequences of words to train the model. However, modern LLMs use extremely larger datasets (GPT-3 training data consists of approximately 500 billion tokens*), and they are trained on diverse data sources from different domains to capture a wide range of language patterns. Bengio et al.'s model is trained on these small datasets, only for the experimental purposes.\n",
        "\n",
        "*: https://en.wikipedia.org/wiki/GPT-3\n",
        "\n",
        " ---\n",
        "\n",
        "**Q5**: Which components of the Bengio et al. (2003) model (if any) can be found in modern LMs?\n",
        "\n",
        "**A**: First and foremost, the part where we get the word embeddings from tokens can be found in modern LMs as well. You can even use the layer `nn.Embedding` in `PyTorch` to train embeddings from scratch. Additionally, modern LMs can also be classified as neural based language models. After Bengio et al. successfully showed that the neural-based models outperform $n$-gram models, the whole paradigm switched to neural networks. However, in contrast, learning the word embeddings is only the first step in modern LLMs, as they consist of lots of other steps like self-attention, feed-forward layers, etc. It is almost impossible to find any modern LM with such a simple architecture proposed by Bengio et al. However, the idea of distributed representations is still valid and used in modern LLMs.\n",
        "\n",
        "---\n",
        "\n",
        "**Q6**: For each section of the Bengio et al. (2003) paper, what are key differences between the way it is written, the included contents, to the BERT paper (Devlin et al., 2019)? What are key similarities? Write max. 2 sentences per section.\n",
        "\n",
        "**A**: Sections of the Bengio et al. (2003) paper:\n",
        "- Abstract\n",
        "- Introduction\n",
        "- Neural Model\n",
        "- Parallel Implementation\n",
        "- Experimental Results\n",
        "- Extensions and Future Work\n",
        "- Conclusion\n",
        "\n",
        "Sections of the BERT paper (Devlin et al., 2019):\n",
        "- Abstract\n",
        "- Introduction\n",
        "- Related Work\n",
        "- BERT\n",
        "- Experiments\n",
        "- Ablation Studies\n",
        "- Conclusion\n",
        "- Appendix\n",
        "\n",
        "\n",
        "1. Abstract: Both papers begin with an abstract that highlights the advantages of their models over previous approaches. However, Bengio et al. do not provide numerical details about their experimental results, while the BERT paper includes specific percentages showing improvements over previous state-of-the-art models.\n",
        "2. Introduction: Introduction part of the Bengio et al. paper incorporates both the motivation behind the model and the related work in the field. The BERT paper provides a high-level description of the model and outlines the main contributions of the paper, leaving the related work to a separate section.\n",
        "3. Neural Model - BERT: In the main sections where the model details are explained, Bengio et al. provide low-level details such as the equations for the neural network, probability distribution calculations, and the gradient descent update rule. In contrast, the BERT paper, published about 15 years later, doesn't delve deeply into the essentials but instead focuses more on how to pre-train the model and fine-tune it for downstream tasks.\n",
        "4. One of the major differences between the two papers is the \"Parallel Implementation\" section. Bengio et al. discuss the parallelization of their model for training on multiple processors using the Message Passing Interface (MPI). In contrast, the BERT paper does not include such a section, thanks to the significant advancements in computational power and the widespread availability of GPUs for training large models (training on GPUs was introduced with [this paper](http://robotics.stanford.edu/~ang/papers/icml09-LargeScaleUnsupervisedDeepLearningGPU.pdf) in 2009).\n",
        "5. In the experiments section, both of the papers follow a similar pattern where they introduce the details of the standard datasets used for evaluation, the results and show that their models outperform the previous state-of-the-art models. However, the BERT paper includes more detailed ablation studies to analyze the impact of different components of the model.\n",
        "6. Bengio et al. provides a section on \"Extensions and Future Work\" where they discuss potential improvements and future research directions. Surprisingly, BERT paper does not include any section on future work, possibly due to the rapid pace of advancements in the field of NLP and the need to publish results quickly.\n",
        "7. Both papers conclude by summarizing the key findings and contributions of their work. Bengio et al. emphasize that \"an important priority of future research should be to improve speed-up techniques.\"*. In contrast, the BERT paper does not mention such a priority, likely because computational resources were not a bottleneck (access to Cloud TPUs).\n",
        "8. Lastly, BERT paper includes an appendix section that provides additional details about the model architecture and hyperparameters used in the experiments. Bengio et al. does not include such an appendix, possibly due to the simplicity of their model compared to the complexity of BERT.\n",
        "\n",
        "\n",
        "*: They ran 5 epochs over 3 weeks using 40 CPUs on the AP News corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
